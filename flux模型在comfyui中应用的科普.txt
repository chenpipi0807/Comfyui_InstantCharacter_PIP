根据对Flux模型在ComfyUI中部署的代码结构、模型加载方式及社区实践的分析，结合多来源的技术文档和开源实现，Flux模型在ComfyUI中**主要采用Transformer格式（PyTorch原生格式）的模型文件**，而非完整的Diffusers格式仓库结构。以下是关键分析及部署要点：

---

### ⚙️ **一、模型结构：Transformer格式为主**
Flux模型在ComfyUI中以**分拆的独立权重文件**形式存在，需分别下载并放置到指定目录，其结构特征如下：
1. **核心模型组件独立存储**：
   - **UNet主模型**：保存为`.safetensors`文件（如`flux1-fp8-dev.safetensors`），存放于`ComfyUI/models/unet`目录。
   - **CLIP文本编码器**：分为`t5xxl_fp8_e4m3fn.safetensors`（FP8量化版）和`clip_l.safetensors`，存放于`ComfyUI/models/clip`。
   - **VAE解码器**：文件`ae.safetensors`存放于`ComfyUI/models/vae`。
   - **ControlNet/LoRA**：如Inpainting ControlNet需重命名后放入`ComfyUI/models/controlnet`，LoRA模型放入`ComfyUI/models/loras`。

2. **与Diffusers格式的差异**：
   - ❌ **无`model_index.json`**：缺少Diffusers格式的核心配置文件，无法通过`DiffusionPipeline.from_pretrained()`直接加载。
   - ❌ **无模块化子目录**：未按Diffusers标准分离`scheduler`、`feature_extractor`等组件，调度器等配置需通过ComfyUI节点手动设置。

3. **加载逻辑**：
   ComfyUI通过**自定义节点**（如`FluxLoader`、`Xlabs Sampler`）加载分拆的权重文件，而非调用Diffusers API。

---

### 🧩 **二、工作流关键组件**
Flux在ComfyUI中的工作流需以下核心节点支持：
1. **模型加载节点**：
   - `FluxModelLoader`：加载UNet主模型（如FP8量化版显存占用12GB）。
   - `DualCLIPLoader`：同时加载T5XXL和CLIP-L文本编码器，支持自然语言与标签类提示词分离处理。
2. **采样与优化节点**：
   - `Xlabs Sampler`：专为Flux设计的采样器，支持动态CFG（推荐值1.0–2.5）。
   - `FluxGuidance`：质量增强节点，替代传统CFG缩放（默认启用，强度3.0）。
3. **扩展功能节点**：
   - `Flux-ControlNet`：适配Depth/Canny等ControlNet模型，需独立下载并重命名。
   - `LoRA Stacker`：支持多LoRA叠加（如细节增强LoRA+艺术风格LoRA）。

---

### 📥 **三、模型下载与部署指南**
#### 1. **必备模型下载清单**
| **组件类型**       | **推荐版本**                     | **下载路径**                                                                 | **存放目录**               |
|--------------------|----------------------------------|-----------------------------------------------------------------------------|----------------------------|
| **UNet主模型**     | `flux1-fp8-dev`（12GB显存）      | [HuggingFace: Kijai/flux-fp8](https://hf-mirror.com/Kijai/flux-fp8)         | `models/unet`             |
| **CLIP文本编码器** | `t5xxl_fp8_e4m3fn.safetensors`   | [HuggingFace: comfyanonymous/flux_text_encoders](https://hf-mirror.com/comfyanonymous/flux_text_encoders) | `models/clip`             |
| **VAE解码器**      | `ae.safetensors`                 | [HuggingFace: black-forest-labs/FLUX.1-dev](https://hf-mirror.com/black-forest-labs/FLUX.1-dev)       | `models/vae`              |
| **ControlNet**     | `Flux-ControlNet-Inpainting`     | [阿里妈妈开源仓库](https://hf-mirror.com/alimama-creative/FLUX.1-dev-Controlnet-Inpainting-Alpha)    | `models/controlnet`       |
| **常用LoRA**       | `Aesthetic10k.safetensors`       | [XLabs-AI Collection](https://hf-mirror.com/XLabs-AI/flux-lora-collection)              | `models/loras`            |

> 注：FP8量化模型显存占用减半（12GB vs 24GB），但需搭配FP8 CLIP版本使用。

#### 2. **典型工作流示例**
- **文生图基础流**：`FluxModelLoader → DualCLIPLoader → KSampler (Xlabs) → VAE Decoder`。
- **ControlNet修复流**：加载Inpainting ControlNet → 连接掩码图像 → 设置`control_strength=0.9`。
- **LoRA混合风格流**：叠加`Extreme Detailer`（权重0.8） + `Mythical Fantasy`（权重0.4）。

---

### ⚡ **四、优化技巧与常见问题**
1. **显存优化**：
   - 使用FP8量化模型（`flux1-fp8-dev` + FP8 CLIP）可将显存需求降至12GB。
   - 启用`--lowvram`模式或Tiled VAE插件处理高分辨率图像。
2. **参数调优**：
   - **ControlNet强度**：0.4–0.8（过高导致过拟合，过低丢失细节）。
   - **动态CFG**：初始阶段CFG=3.0，后期降至1.5避免颗粒感。
3. **错误排查**：
   - **LoRA兼容问题**：FP8主模型需搭配FP16 LoRA，否则显存溢出（需重下载适配版本）。
   - **节点缺失**：工作流需包含`FluxGuidance`节点，否则生成质量下降。

---

### 💎 **总结：Flux在ComfyUI中的定位**
- **格式选择**：Flux采用**Transformer格式分拆权重**，非Diffusers格式，需手动整合组件。
- **优势**：模块化设计便于灵活替换（如CLIP/VAE独立升级），兼容ControlNet/LoRA生态。
- **局限**：缺乏Diffusers的“开箱即用”性，依赖ComfyUI特定节点实现完整流程。

> 部署建议：从[HuggingFace Flux社区](https://hf-mirror.com/black-forest-labs)下载分拆模型，搭配[阿里妈妈ControlNet工作流](https://hf-mirror.com/alimama-creative/FLUX.1-dev-Controlnet-Inpainting-Alpha)可快速实现高级功能。